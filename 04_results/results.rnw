\begin{multicols}{2}
<<>>=
source("../scripts/functions.r")
lidar_tab <- fread("../data/derived/model_data/linearmodels.csv")
@
\section{Overview of Data}

\subsection{LiDAR Point Cloud}

Table \ref{tab:lidartab} indicates that there are likely some points with noise, particularly reflected by the highest intensity value (\Sexpr{max(lidar_tab$Intensity)}) relative to the mean value (\Sexpr{round(mean(lidar_tab$Intensity),0)}). Noise exclusion techniques are described in \cite{fang2004}, this paper takes a simplistic noise filtering technique that aims to solely remove extreme outliers from the observed intensity values in the chosen data.

\subsection{OS Road Geometry}

Class differences, all singlecarriageway roads, therefore likely 60mph. Removed private roads. A roads + B roads + unclassified. Many unnamed roads, only identification possible through \texttt{identifier}, which splits roads by junctions.

Histograms?

\section{Data Preperation}

Covers script 00.

<<fig.show = 'hide'>>=
source("./fig_lp.r")
roads <- roads[6, ]

chm_gg <- ggplot() +
  geom_raster(data = chm, aes(x = x, y = y, fill = value), alpha = 1) +
  geom_sf(data = roads, colour = alpha("red", .3), fill = "black", alpha = 0.1) +
  scale_fill_viridis() +
  coord_sf() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

dsm_lp_gg <- ggplot() +
  geom_raster(data = dsm_lp, aes(x = x, y = y, fill = value), alpha = 1) +
  scale_fill_viridis() +
  coord_fixed() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

las_i_gg <- ggplot() +
  geom_raster(data = metrics, aes(x = x, y = y, fill = imean), alpha = 1) +
  scale_fill_viridis() +
  coord_fixed() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

aerial_gg <- ggplot() +
  geom_raster(data = aerial, aes(x = x, y = y, fill = value), alpha = 1) +
  scale_fill_viridis() +
  coord_fixed() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@

from \ref{fig:fig_buff} \textbf{(b)} intensities values for ground points below the tree are much lower than those that are not below trees, essentially if including returns with higher than 1 return the intensity values are more unreliable.

// find Average intensity values for first and last returns, i.e. show the shadow below trees


<<fig_lp, fig.cap = "LiDAR point clouds for one selected road section aggregated into 1m$^2$ grids, \\textbf{(A)} Base point cloud $Z$ values, \\textbf{(B)} Normalised Point cloud $Z$ values for only last returns ($lpz$) \\textbf{(C)} Normalised Point cloud $Intensity$ values for last return, \\textbf{(D)} Aerial Data combined to 1 band", fig.height = 16>>=
plot_grid(
  chm_gg,
  dsm_lp_gg,
  las_i_gg,
  aerial_gg,
  labels = "AUTO",
  label_size = 20
)
@

\section{Perpendicular Sampling}

Covers 01

Show figure of sample lines. Show the reduction in number of points (memory saving)

<<samplelines>>=
st_read("../data/derived/roads/sample_lines.gpkg")
las <- catalog("../data/point/")
sampled_las <- fread("../data/derived/model_data/sampled_las.csv")

nrow(sampled_las) / las@data$Number.of.point.records * 100
@

\section{Linear Probability Model}

Covers 02

Models: 

\begin{equation}
\begin{aligned}
\mathrm{Road}_{t} = \alpha 
    &+ \beta_{1}  \mathrm{Intensity}_{t} \\
    &+ \beta_{2}  \mathrm{Luminescence}_{t}    \\
    &+ \beta_{3}  \mathrm{Z}_{t} \\
    &+ \beta_{4}  \mathrm{Dist}_{t} + \epsilon
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\mathrm{Road}_{t} = \alpha 
    &+ \beta_{1}  \mathrm{Intensity}_{t} \\
    &+ \beta_{3}  \mathrm{Z}_{t} \\
    &+ \beta_{4}  \mathrm{Dist}_{t} + \epsilon
\end{aligned}
\end{equation}

GLM

and f1, with 90\% quantiles, for better centrelines.

test.
Correlation of lidar variables
<<sampledlas>>=
sampled_las <- fread("../data/derived/model_data/sampled_las.csv")

corr <- sampled_las %>%
  select(c(
    Z, gpstime, Intensity, ReturnNumber, NumberOfReturns,
    ScanDirectionFlag, EdgeOfFlightline, ScanAngleRank,
    road, lum, dists
  ))

# Create a matrix of all correlations between continuous variables
correlation.matrix <- cor(corr, method = "spearman")

# Sort by highest correlations in relation to Unemployed
corResults <- cor.results(correlation.matrix,
  sort.by = "abs.r", data = corr,
  var.name = "road"
)
# Show only two numbers after the decimal
corResults <- format(corResults, digits = 2)

# Combine significance column with r values (asterisks)
corResults$r <- paste(corResults$r, corResults$sig.)

# Drop the columns of the dataframe
corResults <- select(corResults, -c("x", "p.value", "sig."))

# Rename all columns
names(corResults) <- c(
  "Variable", "Rho", "Lower CI †",
  "Upper CI †"
)
@
<<corr_table, results = 'asis'>>=
# Create Table 1
make_table(corResults, # align col 1 left, rest centred
  cap = "Spearman's rank correlation coefficients for all variables in relation to the road outcome variable",
  align = c("l", "c", "c", "c"),
) %>%
  footnote(
    general_title = " ",
    general = c(
      "* Significant at the 0.05 level;",
      "** Significant at the 0.01 level;",
      "*** Significant at the 0.001 level;",
      "† 95% Confidence Interval"
    )
  ) %>%
  row_spec(c(1, 2, 4, 7), bold = T)
@


<<lmdistributions>>=
lm_preds <- fread("../data/derived/model_data/linearmodels.csv") %>% 
select(c(lm1_pred)) %>% 
melt() # longtable

lm1 <- lm_preds[lm_preds$variable == "lm1_pred", ]
lm1_quant <- quantile(lm1$value, .95) %>% 
    as.numeric()
lm1_quant90 <- quantile(lm1$value, .9) %>% 
    as.numeric()
lm1_quant80 <- quantile(lm1$value, .8) %>% 
    as.numeric()

ggplot() +
    geom_histogram(data = lm1, aes(x = value), bins = 100) +
    geom_vline(xintercept = lm1_quant) +
    geom_vline(xintercept = lm1_quant90) +
    geom_vline(xintercept = lm1_quant80) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlim(c(-0.25, .25)) +
  theme_bw()
@

<<>>=
source("./fig_lm.r")
# from logistic regression essential in R (pocket)

lm1 <- mean((sampled_las$lm1_dum == 1) == (sampled_las$road == 1))
lm90 <- mean((sampled_las$lm1_dum90 == 1) == (sampled_las$road == 1))
lm80 <- mean((sampled_las$lm1_dum80 == 1) == (sampled_las$road == 1))

lm_assess <- rbind(lm1, lm90, lm80) %>%
  as.data.frame()
@
<<>>=
make_table(lm_assess, cap = "test")
@

First model (maximal):

//NOTES//
1. Unfiltered vs filtered global maximal model.. Filters include no samples with multiple returns (partial canopy obstruction). excludes far toomany points

2. Compare global models 1/2/3, and generalised linear model using f1.

3. Compare global model 1 (best fit), to individual model using f1.


//for this section see 450 assess2, details on validation etc.

\section{Noise Filtering}

<<>>=
source("./fig_noise.r")
@

Compare quantiles: choose which to use for new centreline extraction.

<<quant_noise>>=
ggRGB(aerial) +
  geom_sf(data = road_lm1[road_lm1$road_id == "road_6", ], colour = "green") +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

ggRGB(aerial) +
  geom_sf(data = road_lm90[road_lm90$road_id == "road_6", ], colour = "green") +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

ggRGB(aerial) +
  geom_sf(data = road_lm80[road_lm80$road_id == "road_6", ], colour = "green") +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@


From above, lm90 gives more accurate centrelines locations. therefore, new centrelines created from the centre of these points.

<<road_lines>>=
source("./fig_roadlines.r")
ggcentcompare <- ggRGB(aerial) +
  geom_sf(data = centrelines, colour = "red", size = 1) +
  geom_sf(data = cent1, colour = "green", size = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
ggcent1 <- ggRGB(aerial) +
  geom_sf(data = sample_lines, colour = "black", alpha = 0.1) +
  geom_sf(data = cent1, colour = "red", alpha = 1) +
  geom_sf(
    data = cent1_las,
    alpha = 0.5, colour = "green"
  ) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@

<<cent1_fig, fig.cap = "test">>=
plot_grid(
  ggcentcompare,
  ggcent1,
  labels = "AUTO",
  label_size = 20,
  ncol = 2
)
@

<<fig_noise, fig.height = 14, fig.cap = "Removed points through filtering">>=
ggRGB(aerial) +
  geom_sf(data = cent_lm1, colour = "red") +
  geom_sf(data = rd_fil, colour = "green") +
  geom_sf(data = cent1, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@

// Manually measure a road width for a reference, i.e. how close to accurate different models are. GLM LM and individual filtered and unfiltered. Find max and min widths and mean, describe what likely causes the differences.

note that individual models do not work at all.


Reveals that centrelines giving inaccurate results when off the side of the road. Taking 90\% quantile gives more of the road segment but more points. 95\% gives more accurate width but less information for road.

individual models enable the use of more reliable p values to filter out some results, however given the centrelines are not accurate this doesn't work. Payoff doesn't seem that good even given proper centrelines.


\section{Final Model Analysis}

<<>>=
source("../scripts/05_analysis.r")
# use final_cent1
widths_0 <- widths_0[widths_0$road_id == "road_6", ] # lm0
widths_1 <- widths_1[widths_1$road_id == "road_6", ] # lmi
widths_2 <- widths_2[widths_2$road_id == "road_6", ] # lm1
widths_3 <- widths_3[widths_3$road_id == "road_6", ] # lm2


# unfiltered
# old centrelines
lm0 <- ggRGB(aerial) +
  geom_sf(data = sample_lines, colour = "black", alpha = 0.1) +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  geom_sf(
    data = widths_0,
    alpha = 0.5, colour = "green"
  ) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

lmi <- ggRGB(aerial) +
  geom_sf(data = sample_lines, colour = "black", alpha = 0.1) +
  geom_sf(data = cent1, colour = "red", alpha = 1) +
  geom_sf(
    data = widths_1,
    alpha = 0.5, colour = "green"
  ) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

lm2 <- ggRGB(aerial) +
  geom_sf(data = sample_lines, colour = "black", alpha = 0.1) +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  geom_sf(
    data = widths_3,
    alpha = 0.5, colour = "green"
  ) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

lm1IND <- ggRGB(aerial) +
  geom_sf(data = sample_lines, colour = "black", alpha = 0.1) +
  geom_sf(data = cent1, colour = "red", alpha = 1) +
  geom_sf(
    data = widths_IND,
    alpha = 0.5, colour = "green"
  ) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@
<<lmquant_plot, fig.env = "figure*", fig.pos = "t", fig.cap = "test">>=
plot_grid(
  lm90,
  lm1,
  glm1,
  labels = "AUTO",
  label_size = 20,
  ncol = 3
)
@

Using width estimates:
<<>>=

norm_widths <- norm_widths %>%
  as.data.frame() %>%
  slice(2:n() - 1)

norm_widths$model <- c("lm 1", "lm 1 (90 quant)", "glm", "lmi", "lm1 (new cent)")
norm_widths <- norm_widths %>%
  select(model, everything())

names(norm_widths) <- c("Model", "Accuracy")
@
<<mtest_tab, results='asis'>>=
make_table(norm_widths, 2, cap = "testing")
@

Using refined centrelines and noise filtered. Using model 


\section{Road Assessment}
<<>>=

final_data <- final_data %>%
  arrange(roadFunction) %>%
  mutate(meanAngle = replace_na(meanAngle, 0))

final_data$road_id <- gsub("road_", "", final_data$road_id)

make_table(final_data,
  cap = "test",
  col_names = c(
    "Road ID",
    "Road Function",
    "Mean Angle",
    "Mean $Z$",
    "Intensity Range",
    "Classified Width",
    "Estimated True Width"
  )
)
@
<<>>=
final_data <- final_data %>%
  select(c(lm1_mean_cent1, meanAngle, meanZ, rangeInt))

cor_tab <- rcorr(as.matrix(final_data), type = "pearson")

p_val <- function(x) {
  symnum(x, corr = FALSE, na = FALSE, cutpoints = c(
    0, 0.001, 0.01, 0.05, 1
  ), symbols = c("***", "**", "*", " "))
}

# keep all zeros for 3 sig figs
cor_tab$r <- as.character(sprintf("%.2f", cor_tab[[1]]))
cor_tab$P <- p_val(cor_tab$P) %>% na.omit()
cor_tab$r[cor_tab$r == "1.00"] <- "" # change diags to blank

# paste p asterisks to values
cor_tab <- paste(cor_tab$r, cor_tab$P)

cor_tab <- cor_tab %>%
  matrix(ncol = 4) %>%
  as.data.frame()

colnames(cor_tab) <- c("Width", "Max Angle", "Max Z", "Int Range")
Variable <- colnames(cor_tab)
cor_tab <- cbind(Variable, cor_tab)
@

<<cor_table, results = 'asis'>>=
# function to convert significance figures into p asterisks
make_table(cor_tab, dig = 2, cap = "Correlation Matrix") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  # footnote for significance
  footnote(
    general_title = " ",
    general = c(
      "*** Significant at the 0.001 level",
      "** Significant at the 0.005 level",
      "* Significant as the 0.01 level"
    ),
  )
@


\section{Widths and Road Quality}

\section{Road Analysis}

// if there is time:

Use width and max bend sharpness to estimate required speed for required stopping distance. Some maths could be involved. Use to produce speed limit assessment, include steepness somehow (speed limits due to stwwpwnss etc)

\end{multicols}
