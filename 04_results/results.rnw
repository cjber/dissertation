<<>>=
source("../scripts/functions.r")
lidar_tab <- fread("../data/point/points.csv")
time_taken <- read.csv("../scripts/time_taken.csv")
time_taken <- time_taken$x
@

As noted in Section \ref{sec:overview}, computation time is considered an important aspect of this analysis, the total time taken for this analysis, including all data preprocessing, perpendicular sample line extraction, LiDAR sample extraction, construction of linear models, and reprocessing of road centrelines, road feature extraction, and further analysis is \Sexpr{round(time_taken, 2)} minutes.

This chapter sill first outline the results relating to data preprocessing, particularly focusing on LiDAR including both the filtering and sampling techniques employed in this methodology. Following this, road classification through various Linear Probability Models are compared, and selected based on both quantitative and qualitative information. The effects of improving the road centreline locations is also explored in relation to the linear models through the production of the road outcome variable.

// outline this results section here.

\section{Data Preprocessing}

Table \ref{tab:lidartab} indicates that there are likely some points with noise, particularly reflected by the highest intensity value (\Sexpr{max(lidar_tab$Intensity)}) relative to the mean value (\Sexpr{round(mean(lidar_tab$Intensity),0)}). As noted in previous LiDAR classification methods, intensity is often subject to noise, therefore a simplistic noise exclusion technique was considered, as described in section \ref{subsec:noise}, presented by \cite{lidR}.

<<hist_lidar, fig.cap="LiDAR Intensity distribution, post noise filtering", fig.showtext=TRUE>>=
font_add_google("Roboto", "roboto")
font_add_google("Roboto Slab", "robotosl")

clean_las <- fread("../data/point/points_clean.csv")

hist_i <- ggplot(clean_las, aes(x = Intensity)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white", bins = 30) +
  geom_density(aes(x = Intensity), fill = "#FF6666", alpha = .2) +
  theme_classic() +
  theme(
    axis.ticks.y = element_blank(),
    axis.line = element_blank(),
    axis.text = element_text(size = 9, family = "roboto"),
    axis.title = element_text(size = 10, family = "robotosl"),
    axis.text.y = element_blank(),
    legend.title = element_blank(),
    legend.position = "bottom",
    strip.background = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA)
  ) +
  labs(x = "Intensity Values", y = "Distribution") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

hist_i
@
<<fig.show = 'hide'>>=
las <- catalog("/home/cjber/drive/uni/envs492/main/data/point")
roads_buff <- read_sf("/home/cjber/drive/uni/envs492/main/data/derived/roads/roads_buff.gpkg", quiet = TRUE)
aerial <- raster("/home/cjber/drive/uni/envs492/main/data/derived/aerial/aerial_crop.tif")

rd_f <- "road_6"
las <- lasclip(las, extent(roads_buff[roads_buff$road_id == rd_f,]))
aerial <- raster::crop(aerial, extent(roads_buff[roads_buff$road_id == rd_f,]))
aerial <- aggregate(aerial, fact = 8)

aerial <- as(aerial, "SpatialPixelsDataFrame")
aerial <- as.data.frame(aerial)
colnames(aerial) <- c("value", "x", "y")

chm <- grid_canopy(las, res = 2, p2r())

chm <- as(chm, "SpatialPixelsDataFrame")
chm <- as.data.frame(chm)
colnames(chm) <- c("value", "x", "y")

las_lp <- lasfilter(las, NumberOfReturns == ReturnNumber)
chm_lp <- grid_canopy(las_lp, res = 2, p2r())

chm_lp <- as(chm_lp, "SpatialPixelsDataFrame")
chm_lp <- as.data.frame(chm_lp)
colnames(chm_lp) <- c("value", "x", "y")

las_lp <- lasground(las_lp, csf())

#### Create Point DSM
# interpolate ground points to create raster dtm. Uses Classification
# very large number of points, therefore idw used as opposed to kriging
dsm_lp <- grid_terrain(las_lp, 2, knnidw(k = 10, p = 2))
# normalise heights using dtm
norm_lp <- lasnormalize(las_lp, dsm_lp)

dsm_lp <- grid_canopy(norm_lp, res = 2, p2r())

dsm_lp <- as(dsm_lp, "SpatialPixelsDataFrame")
dsm_lp <- as.data.frame(dsm_lp)
colnames(dsm_lp) <- c("value", "x", "y")

las_lp <- grid_canopy(norm_lp, res = 2, p2r())

las_lp <- as(las_lp, "SpatialPixelsDataFrame")
las_lp <- as.data.frame(las_lp)
colnames(las_lp) <- c("value", "x", "y")

# Define your own new metrics
myMetrics <- function(z, i) {
  metrics <- list(
    zmean = mean(z), # Mean products of z by intensity
    imean = mean(i)
  )
}

metrics <- grid_metrics(norm_lp, ~ myMetrics(Z, Intensity), res = 2)

metrics <- as(metrics, "SpatialPixelsDataFrame")
metrics <- as.data.frame(metrics)
colnames(metrics) <- c("zmean", "imean", "x", "y")
roads <- roads_buff[roads_buff$road_id == "road_6", ]

chm_gg <- ggplot() +
  geom_raster(data = chm, aes(x = x, y = y, fill = value), alpha = 1) +
  geom_sf(data = roads, colour = alpha("red", .3), fill = "black", alpha = 0.1) +
  scale_fill_viridis() +
  coord_sf() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

dsm_lp_gg <- ggplot() +
  geom_raster(data = dsm_lp, aes(x = x, y = y, fill = value), alpha = 1) +
  scale_fill_viridis() +
  coord_fixed() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

las_i_gg <- ggplot() +
  geom_raster(data = metrics, aes(x = x, y = y, fill = imean), alpha = 1) +
  scale_fill_viridis() +
  coord_fixed() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

aerial_gg <- ggplot() +
  geom_raster(data = aerial, aes(x = x, y = y, fill = value), alpha = 1) +
  scale_fill_viridis() +
  coord_fixed() +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  ), legend.position = "none") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@

<<>>=
one_return <- clean_las %>%
  filter(Classification == 2, NumberOfReturns == 1) %>%
  select(Intensity) %>%
  unlist() %>%
  as.numeric() %>%
  mean()

mult_return <- clean_las %>%
  filter(Classification == 2, NumberOfReturns > 1) %>%
  select(Intensity) %>%
  unlist() %>%
  as.numeric() %>%
  mean()
@

<<fig_lp, fig.cap = "LiDAR point clouds for one selected road section aggregated into 1m$^2$ grids, \\textbf{(A)} Base point cloud $Z$ values, \\textbf{(B)} Normalised Point cloud $Z$ values for only last returns ($lpz$) \\textbf{(C)} Normalised Point cloud $Intensity$ values for last return, \\textbf{(D)} Aerial Data combined to 1 band">>=
plot_grid(
  chm_gg,
  dsm_lp_gg,
  las_i_gg,
  aerial_gg,
  labels = "AUTO",
  label_size = 20,
  ncol = 4
)
@

Following intensity noise filtering, the highest intensity value was now \Sexpr{max(clean_las$Intensity)}, with a mean of \Sexpr{round(mean(clean_las$Intensity), 0)}. Figure \ref{fig:hist_lidar} gives the distribution of Intensity values for all points within the study area, showing two clear spikes in intensity, at at value of around 50, with another around 350.

Figure \ref{fig:fig_lp} gives the results of further LiDAR preprocessing, comparing \ref{fig:fig_lp} \textbf{(A)} and \ref{fig:fig_lp} \textbf{(B)}, shows how last pulse LiDAR filtering allows for the removal of the majority of tree canopies, leaving only ground points that are considered hard surfaces, and as such are the lowest point the laser pulse has penetrated. Additionally, \textbf{(B)} shows how a digital terrain model, created through interpolation techniques using only the base point cloud may be used to normalise the points, giving a digital surface model which only shows the true height of surface objects, without having to consider the variation in lie of the land. Figure \ref{fig:fig_lp} \textbf{(C)} shows that while filtering for last pulse returns may appear to remove much of the canopy, reflected in the $z$ values, the intensity values for points that have penetrated the canopy are lower than those that did not. This particularly creates issues in the distinction between road and non road in neighbouring areas where the intensity "shadow" created removes the distinct difference in intensity. This essentially suggests that for ground points with multiple returns, the intensity values are likely far less reliable for road classification. Further considering this limitation reveals that for ground classified points with a single return the average intensity value is \Sexpr{one_return}, while for ground classified points with multiple returns, the average intensity value is \Sexpr{mult_return}.

\section{Perpendicular Sampling}

<<sample_lines>>=
las <- catalog("../data/point/")
pt_records <- las@data$Number.of.point.records
sampled_las <- fread("../data/derived/model_data/sampled_las.csv")

sample_lines <- st_read("../data/derived/roads/sample_lines.gpkg", quiet = TRUE)
roads <- st_read("../data/derived/roads/roads_line.gpkg", quiet = TRUE)
@


Unlike previous road classification methodologies, this paper aims to focus primarily on road feature extraction, and not the accurate extraction of road locations, as the full open resource provided by Ordnance Survey already exists. Due to this, a sampling methodology was considered for several reasons; sampling the LiDAR point cloud gives a significant reduction in the number of points that need to be processed. Using the 30m buffer from known road locations, and sample line extraction, the number of points from the original LiDAR point cloud for the 1km$^2$ area was reduced from \Sexpr{las@data$Number.of.point.records} to \Sexpr{nrow(sampled_las)} giving a reduction in number of points by \Sexpr{100 - (nrow(sampled_las) / pt_records * 100)}\%. Additionally, including sample lines allowed for filtering based on features of each sample, allowing for samples fully obscured by canopy to be identified through the number of returns, and excluded easily if required.

<<sample_fig, fig.cap="Sample lines extracted based on known road locations">>=
ggplot() +
  geom_sf(data = sample_lines) +
  geom_sf(data = roads) +
  theme_map()
@

\section{Linear Probability Models}

Selected based on literature, and correlation analysis of the variables (\ref{corr_table}), the first model was constructed to include all variables of importance from the LiDAR point cloud, $z$, Intensity, and Number of Returns. Additionally, luminescance from aerial imagery was included, and the minimum distance of a point from the known road centreline location.

<<sampledlas>>=
sampled_las <- fread("../data/derived/model_data/sampled_las.csv")

corr <- sampled_las %>%
  select(c(
    Z, gpstime, Intensity, ReturnNumber, NumberOfReturns,
    ScanDirectionFlag, EdgeOfFlightline, ScanAngleRank,
    road, lum, dists
  ))

# Create a matrix of all correlations between continuous variables
correlation.matrix <- cor(corr, method = "spearman")

# Sort by highest correlations in relation to Unemployed
corResults <- cor.results(correlation.matrix,
  sort.by = "abs.r", data = corr,
  var.name = "road"
)
# Show only two numbers after the decimal
corResults <- format(corResults, digits = 2)

# Combine significance column with r values (asterisks)
corResults$r <- paste(corResults$r, corResults$sig.)

# Drop the columns of the dataframe
corResults <- select(corResults, -c("x", "p.value", "sig."))

# Rename all columns
names(corResults) <- c(
  "Variable", "Rho", "Lower CI †",
  "Upper CI †"
)
@
<<corr_table, results = 'asis'>>=
# Create Table 1
make_table(corResults, # align col 1 left, rest centred
  cap = "Spearman's rank correlation coefficients for all variables in relation to the road outcome variable"
) %>%
  footnote(
    general_title = " ",
    general = c(
      "* Significant at the 0.05 level;",
      "** Significant at the 0.01 level;",
      "*** Significant at the 0.001 level;",
      "† 95% Confidence Interval"
    )
  ) %>%
  row_spec(c(1, 2, 4, 6, 7), bold = T)
@

This first (maximal) model was constructed as;

\begin{equation}
\begin{aligned}
\mathrm{Road}_{t} = \alpha 
    &+ \beta_{1}  \mathrm{Intensity}_{t} \\
    &+ \beta_{2}  \mathrm{Luminescence}_{t}    \\
    &+ \beta_{3}  \mathrm{Z}_{t} \\
    &+ \beta_{4}  \mathrm{Dist}_{t} + \epsilon
\end{aligned}
\end{equation}

As proposed in Chapter \ref{ch:methods}, the road outcome variable was given as points that fell within a 2m buffer of the known road centreline locations. As such, this meant that a fair number of false negative points are expected to have occured, where points outside 2m of a road centreline location and incorrectly classified as non-road. Due to this, the classification of non-road and road was not a simple selection of points that were above a 50\% threshold prediction as being road. To determine an appropriate cutoff for road predictions a histogram was produced which gave insight into the distribution of the linear prediction values (Figure \ref{fig:lmdistributions}).

<<lmdistributions, fig.cap = "Linear Model Probability Distributions for the Maximal Model, showing vertical lines at the 95th, 90th, and 80th quantile of the distribution">>=
lm_preds <- fread("../data/derived/model_data/linearmodels.csv") %>%
  select(c(lm1_pred)) %>%
  melt() # longtable

lm1 <- lm_preds[lm_preds$variable == "lm1_pred", ]
lm1_quant <- quantile(lm1$value, .95) %>%
  as.numeric()
lm1_quant90 <- quantile(lm1$value, .9) %>%
  as.numeric()
lm1_quant80 <- quantile(lm1$value, .8) %>%
  as.numeric()

ggplot(lm_preds, aes(x = value)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white", bins = 30) +
  geom_density(aes(x = value), fill = "#FF6666", alpha = .2) +
  geom_vline(xintercept = lm1_quant, size = 1) +
  geom_vline(xintercept = lm1_quant90, size = 1) +
  geom_vline(xintercept = lm1_quant80, size = 1) +
  theme_classic() +
  theme(
    axis.ticks.y = element_blank(),
    axis.line = element_blank(),
    axis.text = element_text(size = 8),
    axis.text.y = element_blank(),
    legend.title = element_blank(),
    legend.position = "bottom",
    strip.background = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA)
  ) +
  labs(x = "Linear Model Probability", y = "Distribution") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlim(c(-0.25, .25))
@

Figure \ref{fig:lmdistributions} shows that there is a clear separation between the majority of points, and higher probability values. This therefore gives insight into the true divide between true road and non-road points, allowing for a qualitative analysis to select the most appropriate quantile of probability values. Three quantiles were chosen, the 95th, 90th and 80th, as indicated on Figure \ref{fig:lmdistributions}. 

<<quant_selection>>=
sampled_las <- fread("../data/derived/model_data/linearmodels.csv")
road_lm <- fread("../data/derived/model_data/linearmodels.csv") %>%
  as.data.frame() %>%
  st_as_sf(coords = c("X", "Y"), crs = 27700)
roads <- st_read("../data/derived/roads/roads_line.gpkg", quiet = TRUE)
centrelines <- st_read("../data/derived/roads/roads_line.gpkg", quiet = TRUE) %>%
  st_set_crs(27700)
sample_lines <- st_read("../data/derived/roads/sample_lines.gpkg", quiet = TRUE)

# example road section

road_lm <- road_lm[road_lm$road_id == rd_f, ]
sample_lines <- sample_lines[sample_lines$road_id == rd_f, ]
centrelines <- centrelines[centrelines$road_id == rd_f, ] %>%
  st_crop(sample_lines)

jpgs <- Sys.glob("../data/aerial/*.jpg")
jpgs <- lapply(jpgs, brick)
aerial <- lapply(jpgs, function(x) {
  return(tryCatch(crop(x, sample_lines), error = function(e) NULL))
})

aerial <- aerial %>%
  compact() %>%
  brick()
road_lm <- fread("../data/derived/model_data/linearmodels.csv") %>%
  as.data.frame() %>%
  st_as_sf(coords = c("X", "Y"), crs = 27700)
sample_lines <- st_read("../data/derived/roads/sample_lines.gpkg", quiet = TRUE)

road_lm1 <- road_lm[road_lm$lm1_dum == 1, ]
road_lm90 <- road_lm[road_lm$lm1_dum90 == 1, ]
road_lm80 <- road_lm[road_lm$lm1_dum80 == 1, ]


q95 <- ggRGB(aerial) +
  geom_sf(data = road_lm1[road_lm1$road_id == rd_f, ], colour = "green") +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

q90 <- ggRGB(aerial) +
  geom_sf(data = road_lm90[road_lm90$road_id == rd_f, ], colour = "green") +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

q80 <- ggRGB(aerial) +
  geom_sf(data = road_lm80[road_lm80$road_id == rd_f, ], colour = "green") +
  geom_sf(data = centrelines, colour = "red", alpha = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@

<<quant_fig, fig.cap = "Comparison between Linear Prediction Quantiles showing an example road segment">>=
plot_grid(
  q95,
  q90,
  q80,
  labels = "AUTO",
  label_size = 20,
  ncol = 3
)
@

Figure \ref{fig:quant_fig} reveals that qualitatively, the optimal choice for a quantile filtering of the linear probability distribution is likely the 95th quantile (Figure \ref{fig:quant_fig} \textbf{(A)}). However, observation of the southern section of Figure \ref{fig:quant_fig} \textbf{(A)} reveals that inaccurate centreline locations have led to an incomplete linear model analysis. To compensate for this, a further method proposed aims to improve the accuracy of the given road centreline locations. Additionally, Figure \ref{fig:lmdistributions} reveals that for the 95th quantile probability values, shadow from road hedgerows appears to reduce the model accuracy, as noticable towards the top end of the road. For this reason, a second model was constructed for later comparison, which removes the $luminescance$ information provided by the aerial imagery.

\begin{equation}
\begin{aligned}
\mathrm{Road}_{t} = \alpha 
    &+ \beta_{1}  \mathrm{Intensity}_{t} \\
    &+ \beta_{3}  \mathrm{Z}_{t} \\
    &+ \beta_{4}  \mathrm{Dist}_{t} + \epsilon
\end{aligned}
\end{equation}


\section{Corrected Centreline Extraction}

To improve road centreline location accuracy, the 90th quantile results from the first linear probability analysis were used, due to there being a more complete selection of points, but without compromising the true location of roads by including too many outside points.

New road centrelines are given on Figure \ref{fig:cent1_fig}, particular improvements are given where the road curves between two open fields, but the original centreline was given as a straight line, covering the hedgerow, and no road surface.

<<road_lines>>=
cent1 <- st_read("../data/derived/roads/cent_iteration1.gpkg", quiet = TRUE)
cent1 <- cent1[cent1$road_id == "road_6", ]
cent1 <- cent1[cent1$road_id == "road_6", ] %>%
  st_crop(aerial)

ggcent <- ggRGB(aerial) +
  geom_sf(data = centrelines, colour = "red", size = 1) +
  geom_sf(data = cent1, colour = "green", size = 1) +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@



<<>>=
cent0 <- st_read("../data/derived/roads/roads_line.gpkg", quiet = TRUE)
cent0 <- cent0[cent0$road_id == "road_6", ] %>%
  st_crop(aerial)
cent1 <- st_read("../data/derived/roads/cent_iteration1.gpkg", quiet = TRUE)
cent1 <- cent1[cent1$road_id == "road_6", ] %>%
  st_crop(aerial)

cent0_las <- fread("../data/derived/model_data/linearmodels.csv") %>%
  as.data.frame() %>%
  st_as_sf(coords = c("X", "Y"), crs = 27700)
cent1_las <- fread("../data/final_data/cent_lm.csv") %>%
  as.data.frame() %>%
  st_as_sf(coords = c("X", "Y"), crs = 27700)

cent1_las <- cent1_las[cent1_las$road_id == "road_6" & cent1_las$lm1_dum == 1, ]

cent1_fil <- split(cent1_las, cent1_las$sample_id)
cent1_fil <- cent1_fil %>% compact()
cent1_fil <- lapply(cent1_fil, filter_samples)
cent1_fil <- do.call(rbind, cent1_fil)

cent0_las <- cent0_las[cent0_las$road_id == "road_6" & cent0_las$lm1_dum == 1, ]


cent0_fil <- split(cent0_las, cent0_las$sample_id)
cent0_fil <- cent0_fil %>% compact()
cent0_fil <- lapply(cent0_fil, filter_samples)
cent0_fil <- do.call(rbind, cent0_fil)


gg_cent0 <- ggRGB(aerial) +
  geom_sf(data = cent0_las, colour = "red") +
  geom_sf(data = cent0_fil, colour = "green") +
  geom_sf(data = cent0, colour = "red") +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

gg_cent1 <- ggRGB(aerial) +
  geom_sf(data = cent1_las, colour = "red") +
  geom_sf(data = cent1_fil, colour = "green") +
  geom_sf(data = cent1, colour = "red") +
  theme_map() +
  theme(panel.border = element_rect(
    colour = "black",
    fill = NA, size = 1
  )) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
@

<<fig_noisecent, fig.cap = "Comparison between Linear Probability models applied to; (A) OS Road Centrelines, and (B) Derived Centrelines. Green points give classified road locations, red show classified road locations removed through isolation based filtering">>=
plot_grid(
  ggcent,
  gg_cent0,
  gg_cent1,
  labels = "AUTO",
  label_size = 20,
  ncol = 3
)
@

Qualitative comparison between the Linear Probability Model based off the original centreline locations reveals an improvement in overall road detection, particularly towards the edge of roads, while additional samples are achieved in areas which previously had no coverage due to the incorrect centreline placement (Figure \ref{fig:fig_noisecent}). However, it appears that in areas where there are higher levels of linear predictive inaccuracy, the new centrelines are less accurate, thankfully, noise exclusion techniques employed have removed samples that fall within these areas, particualrly noticable at the northern end of Figure \ref{fig:fig_noisecent} \textbf{(B)}. Figure \ref{fig:fig_noisecent} also gives information regarding distance based noise exclusion technique, which has allowed for the exclusion of isolated points accurately on Figure \ref{fig:cent1_fig} \textbf{(A)}.

\section{Final Model Analysis}

For a more comprehensive analysis of the overall suitability of the road width linear probability models, and a comparison between them, a quantitative assessment must be undertaken, to ensure all roads covered in this analysis are considered. For this purpose, the calculated widths were compared with rough width estimates for each road.

<<>>=
road_est <- st_read("../data/osroads/roads_estwidth.gpkg", quiet = TRUE) %>%
  st_drop_geometry() %>%
  na.omit()

# width comparison
widths <- fread("../data/final_data/final.csv") %>%
  select(-c(
    len,
    V1,
    mean_angle,
    max_angle,
    tot_z,
    mean_int,
    range_int,
    tot_pts
  )) %>%
  merge(road_est, by = c("road_id", "roadFunction")) %>%
  na.omit()

normalise_widths <- function(x, y) {
    ifelse(x > y, 100 - ((x/y) * 100 - 100), x/y * 100) 
}

norm_widths <- widths %>%
    mutate_at(3:ncol(widths), ~normalise_widths(., estWidth)) %>% 
    select(-estWidth)

norm_means <- widths %>%
   mutate_at(3:ncol(widths), ~normalise_widths(., estWidth)) %>% 
  summarise_at(vars(1:length(widths)), funs(mean), na.rm = T) %>%
  select(-estWidth)

norm_means$road_id <- "Means:"

norm_widths <- norm_widths %>%
  arrange(desc(lmi_mean), desc(lm2_mean), desc(lm1_mean), desc(lm0_mean)) %>%
  rbind(norm_means)

norm_widths$road_id <- gsub("road_", "", norm_widths$road_id)
@
<<est_widths, fig.cap = "Comparison between different Linear Probability Models and the estimated road width">>=
make_table(norm_widths,
  cap = "Comparison between different linear probability models for each individual road, in relation to estimated true road width",
  col_names = c(
    "ID",
    "Class",
    "LM $i$",
    "LM 1",
    "LM 2",
    "LM 0"
  )
) %>%
  row_spec(nrow(norm_widths), bold = T) %>%
  row_spec(nrow(norm_widths) - 1, hline_after = T)
@

Table \ref{tab:est_widths} gives a normalised comparison between each linear model, and its associated estimated road width. Normalisation was achieved through finding the relative difference in width as a percentage;

\[
\begin{aligned}
    \mathbf{W}_{n} = \frac{\mathbf{W}}{W_{e} \times 100}
\end{aligned}
\]

where $\mathbf{W}_{n}$ is the normalised width, $\mathbf{W}$ is the average width per road derived from the linear model, and $\mathbf{W_{e}}$ is the qualitatively estimated width. Given some widths occasionally were overestimated, to ensure the outcome of this calculation gave a more relative value, any normalised width given a value above 100 was reassigned;

\[
\begin{aligned}
    \mathbf{W}_{n} = 100 - \mathbf{W}_{n}
\end{aligned}
\]

given $\mathbf{W}_{n} > 100$. Table \ref{tab:est_widths} gives insight into the effectiveness of various linear probability models for each road, and road type. While average values all give relative accuracy in the region of 70\%, it appears likely that without adjusting the road centrelines, model probabilities are reduced given \textbf{LM 0} gives the lowest average accuracy in relation to the other three models using improved centrelines for outcome variable creation. Unexpectedly, the roads with the highest accuracy are Minor Roads, rather than B roads. For certain roads, the accuracy of the derived centrelines appear to vastly reduce model accuracy, (For example Road 36; Table \ref{tab:est_widths}).

\section{Road Assessment}
<<final_data, results = 'asis'>>=
sf_est_width <- st_read("../data/osroads/roads_estwidth.gpkg", quiet = TRUE)
road <- st_read("../data/derived/roads/roads_line.gpkg", quiet = TRUE)
final_data <- fread("../data/final_data/final.csv") %>%
  select(-c(
    V1,
    mean_angle,
    mean_int,
    lmi_mean,
    lm0_mean,
    lm1_mean
  ))

final_data <- final_data %>%
  arrange(as.numeric(road_id)) %>%
  mutate(max_angle = replace_na(max_angle, 0)) %>%
  merge(road, by = c("road_id", "roadFunction")) %>%
  mutate(length = as.numeric(st_length(geom)))

final_data$road_id <- gsub("road_", "", final_data$road_id)

summary_data <- final_data %>%
  select(-c(geom, length, tot_pts, len.x, len.y))
@
<<>>=
cor_tab <- rcorr(as.matrix(summary_data[, 3:length(summary_data)]), type = "spearman")

p_val <- function(x) {
  symnum(x, corr = FALSE, na = FALSE, cutpoints = c(
    0, 0.001, 0.01, 0.05, 1
  ), symbols = c("***", "**", "*", " "))
}

# keep all zeros for 3 sig figs
cor_tab$r <- as.character(sprintf("%.2f", cor_tab[[1]]))
cor_tab$P <- p_val(cor_tab$P) %>% na.omit()
cor_tab$r[cor_tab$r == "1.00"] <- "" # change diags to blank

# paste p asterisks to values
cor_tab <- paste(cor_tab$r, cor_tab$P)

cor_tab <- cor_tab %>%
  matrix(ncol = 4) %>%
  as.data.frame()

colnames(cor_tab) <- c("Angle", "$z$", "Range", "Width")
Variable <- colnames(cor_tab)
cor_tab <- cbind(Variable, cor_tab)
@

<<cor_table, results = 'asis'>>=
# function to convert significance figures into p asterisks
make_table(cor_tab, dig = 2, cap = "Correlation Matrix") %>%
  # footnote for significance
  footnote(
    general_title = " ",
    general = c(
      "*** Significant at the 0.001 level",
      "** Significant at the 0.005 level",
      "* Significant as the 0.01 level"
    ),
  )
@

\section{Analysis of Road Features}

<<final>>=
final_norm <- final_data %>%
  select(-c(geom, len.x, len.y)) %>% 
  na.omit()
final_norm$reliability <- final_norm$tot_pts / final_norm$length

range_norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# normalised comparison relative to known
final_norm[, 3:6] <- final_norm[, 3:6] %>%
  mutate_if(is.character, as.numeric) %>%
  mutate(lm2_mean = 1 / lm2_mean) %>%
  lapply(range_norm) %>%
  as.data.frame()

final_norm <- final_norm %>%
  mutate(qual_index = select(., max_angle:lm2_mean) %>% rowSums()) %>%
  mutate(qual_index = 1 - qual_index) %>%
  arrange(desc(qual_index)) %>%
  select(-c(length, tot_pts))

qual_index <- final_norm %>%
  select(road_id, reliability, qual_index)

summary_data <- merge(summary_data, qual_index, by = "road_id") %>%
  arrange(desc(qual_index))
@

<<final_table>>=
make_table(summary_data,
  cap = "Overall Features Extracted from Roads in the Study Area",
  col_names = c(
    "Road ID",
    "Road Function",
    "Max Angle",
    "Total $Z$",
    "Intensity",
    "Width \\textbf{(LM 2)}",
    "Reliability $(\\mathbf{P}_{n} / \\mathbf{L}$)",
    "RQI"
  )
)
@

\subsection{Further Analysis}

<<>>=
roads <- st_read("../data/derived/roads/roads_line.gpkg", quiet = TRUE) %>%
  mutate(len = as.numeric(st_length(geom))) %>%
  st_cast("POINT") %>%
  group_by(road_id) %>%
  summarise(
    num_bend = n() - 1,
    len = mean(len) / 1000,
    roadFunction = unique(roadFunction)
  )

rd_bends <- roads %>%
  mutate(bend_km = num_bend / len) %>%
  st_drop_geometry() %>%
  select(-roadFunction) %>%
  arrange(desc(bend_km))

rd_bends$road_id <- gsub("road_", "", rd_bends$road_id)
make_table(rd_bends,
  cap = "Bends per km",
  col_names = c(
    "Road ID",
    "Number of Bends",
    "Road Length",
    "Bends per km"
  )
)
@
