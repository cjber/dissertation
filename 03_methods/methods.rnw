    \lettrine{T}{his} paper primarily makes use of the free open source statistical language \R{} \citep{parallel}. Managing the large LiDAR datasets from my personal computer was mode possible through the \texttt{lidR} \R{} package \citep{lidR}. Further details regarding the \R{} environment and computer setup used for this paper is given in \textbf{Appendix \ref{a:code}}. All content was written using \LaTeX{} combined with the \texttt{rnoweb} file type \citep{ihaka2011}, for \textit{Literate Programming}\footnote{See \cite{knuth1984}; \textit{“Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”}}. The template is built from scratch but takes much inspiration (and code) from the \href{https://github.com/asardaes/R-LaTeX-Template}{R-LaTeX-Template}.

    All code is hosted on my personal \href{https://github.com/cjber/}{GitHub account}, along with my my complete dotfiles, used in conjunction with the Linux distribution Manjaro, and the i3 window manager. All writing and code was produced using \href{https://neovim.io}{Neovim} with my personal configuration to implement integrated development environment (IDE) style features for writing R code, while also providing essential features for writing in \LaTeX{}. Neovim has the benefit of being both highly customisable, and lightweight, which allows for much lower system utilisation compared with R Studio when working with large datasets. One essential Vim plugin to mention is \href{https://github.com/jalvesaq/Nvim-R}{Nvim-R}, providing an \R{} REPL connection to vim, and other useful functions. 

    Given in Appendix \ref{a:code} are the code snippets utilised in this methodology, for many equations, the relevant code is given as a reference to the appendix location, in the form \textbf{A.x.x}. Due to the nature of the functions used in this analysis, a single function often contains multiple equations, and so a reference to a particular appendix number may be repeated. Additionally, not all functions are referenced in text, so there are gaps in references to functions, and they do not necessarily appear sequentially.

\section{Data}
\label{sec:data}

<<cloud_total>>=
source("../scripts/functions.r")
ctg <- catalog("../data/point/")
number_points <- comma(sum(ctg@data$Number.of.point.records))
@

LiDAR point cloud data was downloaded through the \href{https://data.gov.uk/}{UK Government's open data repository} which uses the \href{http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/}{Open Government Licence}, allowing for:
    \begin{itemize}
        \item Copying, publishing, distributing and transmission of the data
        \item Adaptation of the data
        \item Commercial and Non-commercial use of the information
    \end{itemize}

    LiDAR data used in this paper is available \href{https://data.gov.uk/dataset/977a4ca4-1759-4f26-baa7-b566bd7ca7bf/lidar-point-cloud}{\textsc{here}} under this licence \citep{ukgovernment2019}. This data was given as a compressed LAS file format (\texttt{.laz}), the \R{} package \texttt{lidR} provided the function \texttt{catalog()} which enabled each separate \texttt{.laz} to be combined into one object of class \texttt{LAScatalog}. Analysis on this object could then be split into chunks (selected as 500m$^2$), allowing for multi-core threading to speed up analysis, and a reduction in the memory overhead when reading in data, often a limitation of the \R{} language as objects are stored entirely into memory when read \citep{wickham2014}. The \texttt{LAScatalog} object did not require the compressed \texttt{.laz} files to be read into memory as \texttt{.las} files, meaning memory limitations were far less of a problem.

    Aerial imagery was downloaded through \href{https://digimap.edina.ac.uk/}{Digimap\textsuperscript{\textregistered}
    } which uses the \textit{Aerial Digimap Educational User Licence}, allowing for free use of the data for educational purposes \citep{theuniversityofedinborough2019}.

Road centreline geometries were accessed through the \href{https://www.ordnancesurvey.co.uk/business-and-government/products/opendata.html}{Ordnance Survey Open Data repository} which shares the Open Government licence. These were downloaded in the GeoPackage format (\texttt{.gpkg}) nationally and cropped to the extent of the LiDAR point cloud data.

\section{LiDAR Preprocessing}

The total number of LiDAR points used in this study is \Sexpr{number_points}. All LIDAR data has a vertical accuracy of +/-15cm Root mean square error (RMSE). An overview of the LiDAR data selected for this study is given on Table \ref{tab:lidartab}. The variables of primary interest are:

\begin{itemize}
    \item \textbf{z:} The distance a laser pulse is reflected back to to scanner, calculated by the time taken for a return pulse to be detected.
    \item \textbf{Intensity:} The amplitude of the return pulse, reflected back by the surface terrain or objects.
	\item \textbf{ReturnNumber:} A number of range 1-5, indicating for a point, the corresponding order of a reflected laser pulse. A ReturnNumber of 1 indicates the first return for a pulse (and highest $z$ value), a return number of 5 indicates the last return (and lowest $z$ value).
	\item \textbf{NumberOfReturns:} The number of return pulses for a single laser pulse (maximum of 5).
    \item \textbf{Classification:} A number given to a point indicating a specific numeric classification. Of interest in this study is a classification of 2, indicating a ground point. More information can be found \href{http://desktop.arcgis.com/en/arcmap/10.3/manage-data/las-dataset/lidar-point-classification.htm}{\cite{esri2019}}, which outlines numerical classifications for various vegetation types and man made structures.
\end{itemize}

<<>>=
model <- fread("/home/cjber/drive/uni/envs492/main/data/point/points.csv")
lidar_tab <- model %>% dplyr::select(Z, Intensity, ReturnNumber, NumberOfReturns, ScanDirectionFlag, EdgeOfFlightline, Classification, ScanAngleRank)
tmp <- do.call(
  data.frame,
  list(
    Mean = apply(lidar_tab, 2, mean),
    SD = apply(lidar_tab, 2, sd),
    Min = apply(lidar_tab, 2, min),
    Max = apply(lidar_tab, 2, max)
  )
)
@

<<lidartab, results='asis'>>=
make_table(tmp, cap = "LiDAR Point Cloud Summary Data")
@

\subsection{Last Pulse}

The LiDAR point cloud data used in this paper gives the values for 5 pulse returns. The canopy above roads may be excluded through ignoring early pulses (higher Z values), therefore only the last pulse values for any point are selected, this can be expressed as;

$$
\mathbf{p}_{i}=(l p x, l p y, l p z, l p i),
$$
\begin{flushright}
    \footnotesize{\ref{code:lidr_clean}}
\end{flushright}

\noindent
where $\mathbf{p_i}$ is a single instance of a LiDAR point within the chosen point cloud, $lpx$ is the last pulse $x$ coordinate, $lpy$ the last pulse $y$ coordinate, $lpz$ the last pulse $z$ coordinate, and $lpi$ the last pulse intensity value.

\subsection{Normalisation}

Ground points were classified using the Cloth Simulation Filtering (CSF) algorithm, as described in \cite{zhang2016}. Points were already classified in the data provided, however, as the classification technique was unknown, reclassification was considered necessary. The general implementation simulates the movements of a piece of cloth lying over the inverse of a point cloud, as the point cloud is flipped, the cloth settles beneath ground points, while covering points that lie separate to the ground, essentially forming a digital terrain model (DTM), cloth simulations are described in more detail in \cite{bridson2005} and subsection \ref{subs:dtm}. The CSF algorithm is given;

$$
X(t+\Delta t)=2 X(t)-X(t-\Delta t)+\frac{G}{m} \Delta t^{2},
$$
\begin{flushright}
    \footnotesize{\ref{code:lidr_clean}}
\end{flushright}

where $m$  is the mass of a single LiDAR point (set to 1), $\Delta t$ is the time step between points and $G$ represents the gravity constant. The implementation of this algorithm was given as part of the \texttt{lidR} package.

<<>>=
pts <- fread("../data/point/points.csv")
pts_clean <- fread("../data/point/points_clean.csv")

matches_pts <- merge(pts, pts_clean, by = c("X", "Y")) %>% 
    na.omit()

cchange <- sum(matches_pts$Classification.y == 2) / sum(matches_pts$Classification.x == 2)
@

With the classification of ground points, (given $Classification = 2$), a full DTM may be produced through spatial interpolation of the classified points. This process is called normalisation, and ensures that when extracting height information, any observed values are due to objects on the surface of the terrain, and not due to the lie of the terrain itself. Interpolation uses the inverse distance weighting and $k$ nearest neighbours algorithms to produce the DTM. Nearest neighbours were selected as $k = 10$, with $q = 2$ for the inverse weighting, and used to produce a DTM with a resolution of 1m. This particular technique was selected over more comprehensive methods such as kriging as the number of points is very high, and the small benefit of kriging was considered minimal compared with the increase in computational load. The $z$ values from the DTM were then subtracted from the LiDAR point cloud, leaving a normalised point cloud. Reclassification resulted in an increase in the number of classified ground points by around \Sexpr{round((cchange - 1) * 100, 2)}\%. Reflecting primarily the removal of preexisting classification of other ground objects such as vegetation.

\subsection{Points Extent}

With the normalised last pulse point cloud, the point cloud was clipped to within a 30m extent of each known road location, using the OS road shapefiles. Selecting a 30m extent ensured that even with slight inaccuracy in road location, the road LiDAR points would likely not be excluded. A large number of unimportant points were therefore removed, saving on computational resources. Additionally this extent ensured that both road and non road points were included, but reduced the chance of false positives from occurring as fewer non road points were now included in the analysis.

\begin{flushright}
    \footnotesize{\ref{code:extract_buff}}
\end{flushright}


\subsection{Noise Filtering}\label{subsec:noise}

Intensity noise was filtered through area based outlier detection, measuring the 95th percentile values within a 10m by 10m area, removing all points above the 95\% percentile. This can be expressed as;

\[
    \begin{aligned}
\mathbf{c}_{i} = \Big(\mathbf{p}_i \in \Big[\frac{95}{100} \times \mathbf{p}_{i_{lpi}}\Big]\Big) \\
A\left(\mathbf{c}_{\mathbf{i}}\right)=10 m^{2}
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:las_filter_noise}}
\end{flushright}

\noindent
where $\mathbf{c_i}$ represents a 10m$^2$ selection of LiDAR points, where each point $(\mathbf{p_i})$ has an intensity value within the 95\% percentile intensity for all points $(\mathbf{p}_{i_{lpi}})$ within $\mathbf{c_i}$.

\subsection{LiDAR Catalog}

As mentioned above, objects of class LiDAR catalog enabled more efficient processing through allowing the LiDAR point cloud to be processed in predefined batch sizes. Considering a collection of processed LiDAR points, with last pulse, normalised, clipped to 30m road extents, and intensity noise filtered, points were grouped into 500m$^2$ areas;

\[
\begin{aligned}
\mathbf{C}_{i}=\left\{\mathbf{c}_{1}, \mathbf{c}_{2}, \ldots, \mathbf{c}_{N}\right\} \\
A\left(\mathbf{C}_{\mathbf{i}}\right)=500 m^{2} \\
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:las_filter_noise}}
\end{flushright}

And each 500$m^2$ area collectively represents the overall processed point cloud;

\[
\begin{aligned}
\mathbf{S}=\left\{\mathbf{C}_{1}, \mathbf{C}_{2} \ldots, \mathbf{C}_{N}\right\}
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:comb_ctg}}
\end{flushright}

\section{Road Analysis}
\label{sec:road-extraction}

The next step is to combine the preprocessed LiDAR data with OS road shapefiles, preprocessed LiDAR data, and aerial imagery to obtain a set of criteria to assess the chosen road network. A summary of the information provided alongside OS road shapefiles is given on Table \ref{tab:roadstab}. While both \texttt{roadNumberTOID} and \texttt{roadNameTOID} do provide true road identification for many roads, this was not true for each road in the area chosen. Due to this, it was impossible to identify what could be considered an individual road, meaning a "road" will now be defined as indicated on \ref{fig:area_map}, selected based on the shapefile geometry provided. Criteria to be assessed for each road are;


\begin{itemize}
    \item Road Width
    \item Bend Sharpness
    \item Road Steepness
    \item Surface Quality
\end{itemize}

<<>>=
roads_info <- fread("/home/cjber/drive/uni/envs492/main/data/osroads/roads_info.csv") %>% 
select(-c(geometry, length_uom, name1_lang, name2_lang))

funcs <- unique(roads_info$roadFunction)

tmp <- do.call(
  data.frame,
  list(
    Example = apply(roads_info, 2, head, 1)
  )
)
@

<<roadstab, results='asis'>>=
make_table(tmp, cap = "OS Roads Data Summary")
@

The roads in this paper consist of these functions; 

\begin{itemize}
<<results = 'asis'>>=
cat(paste("\\item", funcs), sep = "\n")
@
\end{itemize}

\noindent with B roads considered classified roads, and other functions considered unclassified. All roads are single carriageway, and so for the purpose of this analysis it is assumed they likely have the default national speed limit of 60mph. All private roads were removed, as were roads with a length of less than 50m, often those clipped by the extent of the LiDAR data.

\subsection{Road Sampling}

The LiDAR point cloud data was sampled at regular 10 meter intervals for each road, perpendicular to the road direction, ensuring that when road direction changed, the sampling locations remained perpendicular. Each road was first split into nodes at which road direction changed, with a single road consisting of multiple nodes with coordinates $xy$, indicating a point along a road where the road direction changed. From this, points at 10 meter intervals beginning at the start of a road, (considered Node 1; $\mathbf{N}_1$), until the next node along the road ($\mathbf{N}_2$). Each 10 meter interval was assigned a point with $xy$ coordinates, to calculate these points along each line between two nodes ($\mathbf{N}_{i} = (x_i, y_i)$ and $\mathbf{N}_{i+1} = (x_{i+1},y_{i+1})$), first the total $xy$ distances between the two nodes was calculated;

\[
\begin{aligned}
    |x| =& x_{i+1} - x_i \\
    |y| =& y_{i+1} - y_i,
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

\noindent along with the euclidean distance between these nodes;


\[
\begin{aligned}
    d(\mathbf{N}_{i},\mathbf{N}_{i+1}) = \sqrt{(x_{i+1} - x_{i})^2 + (y_{i+1} - y_{i})^2}.
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

To find a point ($\mathbf{p}_i$) along these two nodes, at an interval distance $I_k$, can be calculated; 

\[
\begin{aligned}
    px =& \frac{x_{i} + |x|}{d(\mathbf{N}_{i},\mathbf{N}_{i+1})} \times I_k \\
    py =& \frac{y_{i} + |y|}{d(\mathbf{N}_{i},\mathbf{N}_{i+1})} \times I_k,
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

where $I_k$ is the interval value which increases by 10 meters until the length of the total distance of the node, given . Additionally 
\noindent giving a point $p_i = (px,py)$ at a distance $I$ from $N_i$ in the direction of $N_{i+1}$. Where $I_k$ is the increment, which increases by 10m given $I_{1} = 10, I_{2} = 20, \dots, I_{k}>0$ and $i \geq 2$. From a point $p_i$ at position $I_k$ between two nodes $N_i$ and $N_{i+1}$, to create a perpendicular sample, first two points at a perpendicular distance $\delta$ from the bearing angle of the two nodes, at a point $p_i$ were created, with $\delta$ selected as 30m. First the euclidean distance from $N_{i+1}$ to the point $p_i$ was calculated;

\[
\begin{aligned}
    d(N_{i+1},p_i) = \sqrt{(x_{i+1} - px)^2 + (y_{i+1} - py)^2}
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

\noindent with this perpendicular distance, the value required for each $xy$ coordinate to achieve a distance of $\delta$ from a point $p_i$ may be calculated by;

\[
\begin{aligned}
    \delta_x = \frac{\delta}{d(N_{i+1},p_i)} \times (x_{i+1} - px_i) \\
    \delta_y = \frac{\delta}{d(N_{i+1},p_i)} \times (y_{i+1} - py_i)
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

then to create perpendicular points at length $\delta$ from the point $p_i$ between the nodes.

\[
\begin{aligned}
    P_{perp}=\left\{\begin{array}{ll}
            px_i + \delta_y, & py_i - \delta_x \\
        px_i - \delta_y, & py_i + \delta_x
\end{array}\right.
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

\noindent

\[
\begin{aligned}
    I_k &= (L_k + 10) - T_k \\
    L_1 &= 10 \\
    T_k &= T_{k-1}  + E
\end{aligned}
\]

where $L_k$ is the distance from the start of the node to the current sample line position, $T_k$ is total length of all nodes within a road from the start of the road and $E$ is the euclidean distance between two nodes.

To calculate the sample locations, perpendicular to the roads, first the $x$ and $y$ euclidean distances from a reference point $p_k$ and the end of the current node $n_{k+1}$ were calculated;

\[
\begin{aligned}
    len &= \sqrt{(n_{k+1} - p_k)} \\
    p_i &= (30 / len) \times (n_{k+1} - p_k)
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:perp}}
\end{flushright}

from this, the lengths were inverted, giving two points ($p_i$) of distance $30$ from $p_k$. Inversion was achieved by subtracting the given $len_n$ $x$ value from $p_k$, while adding the $len_n$ $y$ value, and vice versa. These perpendicular sample lines were buffered to a width of 2m, giving a total area of 120m$^2$ per sample. These were spatially joined to the existing LiDAR point cloud data, removing any points that fell outside the sample lines.

\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

\subsection{Aerial Imagery}

With the perpendicular sample lines extracted for the length of every road, to assist with the prediction of correct road locations, true colour aerial imagery was included. This imagery was first converted from three band RGB raster images, to a single-band grey-scale raster brick with values ranging 0 to 255, considered to be luminescence information.

$$
luminescence = \frac{Band_1 + Band_2 + Band_3}{3}
$$
\begin{flushright}
    \footnotesize{\ref{code:lm_compute}}
\end{flushright}

\section{Linear Probability Models and Road Width}

To classify road and non-road, linear models were constructed and compared to assess effectiveness. A maximal approach was chosen, selecting all appropriate predictor variables, iterating through models by removing variables that did not significantly influence the model outcome, or created noise.

An additional variable $Dist$ was created and included, representing the shortest distance from a point to the centreline of the road it is associated with, considering that road points should be weighted more towards points that are closer to the centre-point of the road.

Linear probability models essentially follow the same formula as a linear regression model:

\[
\begin{aligned}
Y_{i}=\beta_{0}+\beta_{1}+X_{1 i}+\beta_{2} X_{2 i}+\cdots+\beta_{k} X_{k i}+u_{i}
\end{aligned}
\]

\noindent but given a binary outcome variable $Y_i$, this is considered to be a linear probability model, taking the form;

\[
\begin{aligned}
E\left(Y | X_{1}, X_{2}, \ldots, X_{k}\right)=P\left(Y=1 | X_{1}, X_{2}, \ldots, X_{3}\right)
\end{aligned}
\]

\noindent where;

\[
\begin{aligned}
P\left(Y=1 | X_{1}, X_{2}, \ldots, X_{k}\right)=\beta_{0}+\beta_{1}+X_{1 i}+\beta_{2} X_{2 i}+\cdots+\beta_{k} X_{k i}
\end{aligned}
\]

\noindent $\beta_j$ therefore may be interpreted as the change in the probability that $Y_i = 1$, with all other predictor variable constant. $\beta_j$ may be estimated using Ordinary Least Squares regression \citep{hanck2019}. 

Likelihood values from the predictions gave a range of numerical values, points that fell below a certain threshold were removed, leaving only points that were most likely correctly identified as road points. This threshold was assess qualitatively through both observation of the distribution of probability ranges for each model, and results gained through different thresholds. Considering a threshold $x$, this may be expressed as;

\[
\begin{aligned}
\mathbf{S} &= \Big(\mathbf{p}_i \in \Big[\frac{x}{100} \times \mathbf{p}_{i_{lm}}\Big]\Big), \\
\end{aligned}
\]

\noindent where $\mathbf{S}$ is the total point cloud.

Further qualitative assessment of the results revealed that some points considered to be noise were still present, but often isolated. To ensure no isolated points were present, the minimum distance between each point, and the nearest neighbouring point was checked, if a single point was considered isolated, with over 1m between it and any other point, it was removed. This may be expressed as;

\[
\begin{aligned}
    \mathit{D} &= \sqrt{\delta xi^{2} + \delta y^{2}} \\
    \mathbf{S} &= (\mathbf{p}_{i}\in [\mathit{D} \leq 1]),
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:filter_samples}}
\end{flushright}

\noindent given $\mathit{D}$ is the minimum distance between a point and any other point.

$\mathbf{S}$ now gave of a collection of predicted road points for each sample line along a road segment, with noise removed.  To obtain road widths from these points, the maximum distance between two points in a particular sample was determined, these points were kept and all others removed. A linear section of road with two samples resembles Figure \ref{fig:sample_points}.

\begin{flushright}
    \footnotesize{\ref{code:max_dist}}
\end{flushright}


\begin{figure}[htbp]
    \centering
\begin{tikzpicture}[scale=2]

\draw[-, very thick, color = gray] (0,0.5) -- (0,4);

\filldraw (-1,1) circle[radius=1pt] node [below left] {$\mathbf{A}_1$};
\draw[-] (-1,1) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$h_\mathbf{A}$} (1,2);
\draw[dashed] (-1,1) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$b_\mathbf{A}$} (1,1) ;
\draw[dotted] (1,1) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$a_\mathbf{A}$} (1,2);
\filldraw (1,2) circle[radius=1pt] node [above right] {$\mathbf{A}_2$};

\draw(.85,1)--(.85,1.15)--(1,1.15); % right angle

\draw[-] (-.6,1) arc (0:55:.2);
\node[] at (-.5,1.12)  {$\theta_\mathbf{A}$};

\filldraw (-1,4) circle[radius=1pt] node [below left] {$\mathbf{B}_2$};
\draw[-] (-1,4) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$h_\mathbf{B}$} (1,3);
\draw[dashed] (-1,3) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$b_\mathbf{B}$} (1,3);
\draw[dotted] (-1,3) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$a_\mathbf{B}$}(-1,4);
\filldraw (1,3) circle[radius=1pt] node [above right] {$\mathbf{B}_1$};

\draw(-.85,3)--(-.85,3.15)--(-1,3.15); % right angle

\draw[-] (.6,3.2) arc (135:200:.2);
\node[] at (.4,3.12)  {$\theta_\mathbf{B}$};
\end{tikzpicture}
\caption{Road LiDAR points at maximum distance apart for each sample location. Showing two example sample locations ($\mathbf{A}$ and $\mathbf{B}$), road centreline represented by the thick grey line. True road width is indicated by the dashed lines $b_\mathbf{A}$ and $b_\mathbf{B}$, considered to be the adjacent side of a triangle in relation to be bearing angles between the first and second point of each sample, $\theta_\mathbf{A}$ and $\theta_\mathbf{B}$. The distance between the two points per sample are considered to give the hypotenuse length of a triangle ($h_\mathbf{A}$ and $h_\mathbf{B}$).}\label{fig:sample_points}
\end{figure}

To determine the width of the road, given the two selected points at every node with a maximum distance between them, trigonometry could be used to find the adjacent line length ($a_\mathbf{K}$; where $\mathbf{K}$ is a single sample location), perpendicular to the road segment. considering the distance between the two points to be the hypotenuse ($|\mathbf{K}_1\mathbf{K}_2|$) of a right angled triangle (Figure \ref{fig:sample_points}). The average width for each road identifier was then found, for a sample location $\mathbf{K}$, this may be expressed using trigonometry as;

\[
\begin{aligned}
    b_\mathbf{K} =& |\mathbf{K}_1\mathbf{K}_2| \times cos(\theta_\mathbf{K})
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:opposite_length}}
\end{flushright}

\noindent where $b_\mathbf{K}$ gives the true predicted of a road for the sample $\mathbf{K}$. With a complete set of calculated road widths for each sample, any width above 8m was removed, in addition to any width below 2m, under the assumption that a width calculated outside these limits would be caused due to noise or inaccuracy.

There is the possibility that the maximum distance between two points does not provide the maximum perpendicular distance across a road section. Such a situation would arise given a triangle formed that is has an opposite length ($a_\mathbf{K}$) above the adjacent length ($b_\mathbf{K}$). However, given the maximum opposite line length between two points in a sample line is 2m, (each sample is 2m by 60m), for any line where the opposite length is greater than the adjacent length, the adjacent length must therefore be below 2m and thus, the road width calculated from this sample is removed during the filtering process. Such a situation would require a larger opposite length than adjacent length to produce a hypotenuse length in excess of an alternative hypotenuse between two points, given it would have to be taller than it is wide.

\subsection{Improved Road Centrelines}

During the analysis of the linear models, it was noted that road centrelines were often inaccurate, giving road outcome values that were not representative of the road surface. In an attempt to adjust for this, road centrelines were adjusted based on the centre location of road points in each sample, classified through an initial linear probability model. The mid point between two points $\left(x_{1}, y_{1}\right)$ and $\left(x_{2}, y_{2}\right)$ can be expressed as;

$$
\left(\frac{x_{1}+x_{2}}{2}, \frac{y_{1}+y_{2}}{2}\right)
$$
\begin{flushright}
    \footnotesize{\ref{code:mid_pts}}
\end{flushright}


With the mid point of each classified road sample, these were than joined for form new centrelines, and a further linear probability model was ran and compared, to determine the effectiveness of this technique.
\begin{flushright}
    \footnotesize{\ref{code:true_cents}}
\end{flushright}

Given the improvement in road centreline locations, it was considered feasible to construct linear probability models individually for each sample location on each road. This technique would allow for per road variation in material type or quality, and potentially reduce the amount of noise brought in from inaccurate centrelines. Preliminary testing of this method revealed that it was essential to remove any sample containing tree canopy, as given the small number of points, the predictions were based off points that misrepresented true road points. Additionally, individual linear models allowed for some filtering through $p$ values, which was not achievable through global linear models, given the very large number of points. 

\subsection{Final Model Analysis}

// talk about lm beta coefficients here, limitations of p valeus nmaybe

To aid with model interpretability, the direct comparison between each variable in the analysis was enabled through centering and scaling through the use of beta coefficients \cite{peterson2005};

$$
\beta_{p}=\frac{\operatorname{Cov}\left(r_{p}, r_{b}\right)}{\operatorname{Var}\left(r_{b}\right)}
$$




\section{Road Angles}

The angle of bends in roads were identified through the nodes produced in the creation of the road shapefiles. First the road linestrings were split into points, with coordinates representing each node within a road, a point at which the orientation of the linestring is altered.

The bend in a road was considered to be the \textit{bearing angle} $\theta$, from a point $A\left(a_{x}, a_{y}\right)$ to a point $B\left(b_{x}, b_{y}\right)$, with the angle measured in a clockwise direction from north. This is represented on Figure \ref{fig:bearingang1}.

    \begin{figure}[htbp]
        \centering
\begin{tikzpicture}[scale=7]
\filldraw (.2,.4) circle[radius=.3pt] node [below left] {$A$};

\draw[-] (0.1,.4) -- (0.3, .4);
\draw[-] (0.2,.3) -- (0.2, .5) node[above] {$N$};

\filldraw (.8,.7) circle[radius=.3pt] node [above right] {$B$};

\draw[-] (0.2,.4) -- (0.8, .7);

    \draw[<-] (0.28,.44) arc (45:90:.111);
    \node[] at (.25,.5)  {$\theta$};
\end{tikzpicture}
\caption{Bearing Angle Between Road Segments}\label{fig:bearingang1}
\end{figure}

To find the angle $\theta$, the point $B$ can be represented into relation to point $A$ as;

$$
\left(b_{x}, b_{y}\right)=\left(a_{x}+r \sin \theta, a_{y}+r \cos \theta\right)
$$

Where $r$ is the length of the line segment $AB$. Rearranging the equation for $\theta$ gives;

$$
    \tan \theta=\frac{b_{x}-a_{x}}{b_{y}-a_{y}}
$$

This equation can be rewritten to calculate the value of $\theta$ using the $\mathit{atan2}$ function;

$$
\hat{\theta}=\mathrm{atan} 2\left(b_{1}-a_{1}, b_{2}-a_{2}\right) \in[-\pi, \pi]
$$


Finally the bearing angle $\theta \in[0,2 \pi]$ is given as;

\[
    \begin{aligned}
\theta=\left\{\begin{array}{ll}{\hat{\theta},} & {\hat{\theta} \geq 0} \\
{2 \pi+\hat{\theta},} & {\hat{\theta}<0}\end{array}\right.
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:road_angles}}
\end{flushright}

With the bearing angle of the first line segment $AB$ for a particular road, the change in orientation of the second road segment $BC$ may be given as $\theta_{2} = \theta_{1} - \theta_{0}$, with additional nodes following the pattern $\theta_{k} = \theta_{N(N+1)} - \theta_{(N-1)N}$.

    \begin{figure}[htbp]
        \centering
\begin{tikzpicture}[scale=7]

\filldraw (.2,.4) circle[radius=.3pt] node [below left] {$A$};

\draw[-] (0.1,.4) -- (0.3, .4);
\draw[-] (0.2,.3) -- (0.2, .5) node[above] {$N$};

\filldraw (.8,.7) circle[radius=.3pt] node [above left] {$B$};
\filldraw (.9,1) circle[radius=.3pt] node [above right] {$C$};

\draw[-] (0.2,.4) -- (0.8, .7);
\draw[-] (0.8,.7) -- (.9, 1);

    \draw[<-] (0.28,.44) arc (45:90:.111);
    \node[] at (.25,.5)  {$\theta_{1}$};

\draw[dotted] (0.8,.7) -- (1, .8);

    \draw[<-] (0.9,.75) arc (45:90:.1);
    \node[] at (.88,.8)  {$\theta_{2}$};
\end{tikzpicture}
\caption{Bearing Angle Between two Road Segments}
\label{fig:bearingang2}
\end{figure}

For each road the, maximum bearing angle between two nodes was selected, as well as the average bearing angle for a certain road.

\section{Road Node Elevation Change}

The elevation change between two road node points was calculated by first selecting non-normalised LiDAR points at a geometric node within a 1m$^2$ area. Nodes were obtained from the OS road data, as point coordinates obtained when the linestrings were split. LiDAR points were then filtered by those only classified as ground, and with only a single return, to reduce the likelihood of inaccurate $z$ values from canopy or other vegetation and vehicles. The mean $z$ value of points were found for each node, and elevation change between each node was calculated. For each road, the total elevation change per kilometer was calculated.

\section{Surface Quality}

Surface quality was assessed roughly through the range in intensity values found in each known road point, and the average number of returns for a road for sample lines not obstructed by canopy, indicated by single returns.

\section{Estimate of True Widths}

QGIS \citep{QGIS} used to measure roughly the width at various points along each road using 25cm resolution aerial imagery. With the widths, the results of each model was compared to assess model accuracy. Each width was normalised to allow comparison between each road, and to give a final average accuracy value. Normalisation was achieved through finding the relative difference in width as a percentage;

\[
\begin{aligned}
    \mathbf{W}_{n} = \frac{\mathbf{W}}{W_{e} \times 100}
\end{aligned}
\]

where $\mathbf{W}_{n}$ is the normalised width, $\mathbf{W}$ is the average width per road derived from the linear model, and $\mathbf{W_{e}}$ is the qualitatively estimated width. Given some widths occasionally were overestimated, to ensure the outcome of this calculation gave a relative value, any normalised width given a value above 100 was reassigned;

\[
\begin{aligned}
    \mathbf{W}_{n} = 100 - \mathbf{W}_{n}
\end{aligned}
\]

given $\mathbf{W}_{n} > 100$. 

\section{Road Quality Assessment}

To provide a method for direct comparison between each road, the extracted features are normalised and combined as one to produce the Road Quality Index (RQI).

Normalisation of each road feature was achieved through a simple range normalisation;

\[
\begin{aligned}
m \mapsto \frac{m-r_{\min }}{r_{\max }-r_{\min }}
\end{aligned}
\]

Where $r_{\text {min }}$ denotes the minimum of the range of a variable,  $r_{\text {max }}$ denotes the maximum of the range of a variable, and $m \in\left[r_{\text {min }}, r_{\text {max }}\right]$ denotes the variable to be scaled. As an increase in road width is associated with a higher quality road, as opposed to larger values of each other variable indicating a poorer quality road, the width values were first inversed before normalisation. An additional variable, relitability was presented in addition to the RQI, which gives a value for the number of points per road length, ($P_n/L$), allowing for some information regarding the density of sample points to be considered in analysis.

Following normalisation, the sum of all normalised variables for a particular road were taken, and subtracted from 1 to give positive values indicating better quality roads, and lower values indicating lower quality roads. The value obtained from this was called the Road Quality Index, presented in full on Table \ref{tab:rqi}.
