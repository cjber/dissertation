\begin{multicols}{2}
    This paper primarily makes use of the free open source statistical language \R{} \citep{rcoreteam2019}. Managing the large LiDAR datasets from my personal computer was mode possible through the \texttt{lidR} \R{} package \citep{roussel2019}. Further details regarding the \R{} environment and computer setup used for this paper is given in \textbf{Appendix \ref{a:code}}. All content was written using \LaTeX{} combined with the \texttt{.rnoweb} file type \citep{ihaka2011}, for \textit{Literate Programming}\footnote{See \cite{knuth1984}}. The template is built from scratch but takes much inspiration (and code) from the \href{https://github.com/asardaes/R-LaTeX-Template}{R-LaTeX-Template}.

    All code is hosted on my personal \href{https://github.com/cjber/}{GitHub account}. Also hosted are my complete dotfiles, used in conjunction with the Linux distribution Manjaro, with the i3 window manager. All writing and code was produced using \href{https://neovim.io}{\texttt{neovim}} with my personal configuration to implement integrated development environment (IDE) style features for writing R code, while also providing essential features for writing in \LaTeX{}. \texttt{neovim} has the benefit of being both highly customisable, and lightweight, which allows for much lower system utilisation compared with R Studio when working with large datasets. One essential \texttt{vim} package to mention is the \href{https://github.com/jalvesaq/Nvim-R}{Nvim-R} vim package.

    Given in Appendix \ref{a:code} are the code snippets utilised in this methodology, for many equations, the relevent code is given as a reference to the appendix location.

\section{Data}
\label{sec:data}

<<cloud_total>>=
source("../scripts/functions.r")
ctg <- catalog("../data/point/")
number_points <- comma(sum(ctg@data$Number.of.point.records))
@

LiDAR point cloud data was downloaded through the \href{https://data.gov.uk/}{UK Government's open data repository} which uses the \href{http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/}{Open Government Licence}, allowing for:
    \begin{itemize}
        \item Copying, publishing, distributing and transmission of the data
        \item Adaptation of the data
        \item Commercial and Non-commercial use of the information
    \end{itemize}

    LiDAR data used in this paper is available \href{https://data.gov.uk/dataset/977a4ca4-1759-4f26-baa7-b566bd7ca7bf/lidar-point-cloud}{\textsc{here}} under this licence \citep{ukgovernment2019}. This data was given as a compressed LAS file format (\texttt{.laz}), the \R{} package \texttt{lidR} provided the function \texttt{catalog()} which enabled each separate \texttt{.laz} to be combined into one object of class \texttt{LAScatalog}. Analysis on this object could then be split into chunks (selected as 500m$^2$), allowing for multi-core threading to speed up analysis, and a reduction in the memory overhead when reading in data, often a limitation of the \R{} language as objects are stored entirely into memory when read \citep{wickham2014}. The \texttt{LAScatalog} object did not require the compressed \texttt{.laz} files to be read into memory as \texttt{.las} files, meaning memory limitations were far less of a problem.

    Aerial imagery was downloaded through \href{https://digimap.edina.ac.uk/}{Digimap\textsuperscript{\textregistered}
    } which uses the \textit{Aerial Digimap Educational User Licence}, allowing for free use of the data for educational purposes \citep{theuniversityofedinborough2019}.

Road centreline geometries were accessed through the \href{https://www.ordnancesurvey.co.uk/business-and-government/products/opendata.html}{Ordnance Survey Open Data repository} which shares the Open Government licence. These were downloaded in the GeoPackage format (\texttt{.gpkg}) nationally and cropped to the extent of the LiDAR point cloud data.

\section{LiDAR Preprocessing}

The total number of LiDAR points used in this study is \Sexpr{number_points}. All LIDAR data has a vertical accuracy of +/-15cm Root mean square error (RMSE). An overview of the LiDAR data selected for this study is given on Table \ref{tab:lidar}. The variables of primary interest are:

\begin{itemize}
    \item \textbf{Z:} The distance a laser pulse is reflected back to to scanner, calculated by the time taken for a return pulse to be detected.
    \item \textbf{Intensity:} The amplitude of the return pulse, reflected back by the surface terrain or objects.
	\item \textbf{ReturnNumber:} A number of range 1-5, indicating for a point, the corresponding order of a reflected laser pulse. A ReturnNumber of 1 indicates the first return for a pulse (and highest $z$ value), a return number of 5 indicates the last return (and lowest $z$ value).
	\item \textbf{NumberOfReturns:} The number of return pulses for a single laser pulse (maximum of 5).
    \item \textbf{Classification:} A number given to a point indicating a specific numeric classification. Of interest in this study is a classification of 2, indicating a ground point. More information can be found \href{http://desktop.arcgis.com/en/arcmap/10.3/manage-data/las-dataset/lidar-point-classification.htm}{here}, which outlines numerical classifications for various vegetation types and man made structures.
\end{itemize}

<<>>=
model <- fread("/home/cjber/drive/uni/envs492/main/data/point/points.csv")
lidar_tab <- model %>% dplyr::select(Z, Intensity, ReturnNumber, NumberOfReturns, ScanDirectionFlag, EdgeOfFlightline, Classification, ScanAngleRank)
tmp <- do.call(
  data.frame,
  list(
    Mean = apply(lidar_tab, 2, mean),
    SD = apply(lidar_tab, 2, sd),
    Min = apply(lidar_tab, 2, min),
    Max = apply(lidar_tab, 2, max)
  )
)
@

<<lidartab, results='asis'>>=
make_table(tmp, cap = "LiDAR Point Cloud Summary Data") %>%
  kable_styling(latex_options = "HOLD_position")
@



\subsection{Last Pulse}

The LiDAR point cloud data used in this paper gives the values for 5 pulse returns. The canopy above roads may be excluded through ignoring early pulses (higher Z values), therefore only the last pulse values for any point are selected, this can be expressed as;

$$
\mathbf{p}_{i}=(l p x, l p y, l p z, l p i),
$$
\begin{flushright}
    \footnotesize{\ref{code:lidr_clean}}
\end{flushright}

where $\mathbf{p_i}$ is a single instance of a LiDAR point within the chosen point cloud, $lpx$ is the last pulse $x$ coordinate, $lpy$ the last pulse $y$ coordinate, $lpz$ the last pulse $z$ coordinate, and $lpi$ the last pulse intensity value.

\subsection{Normalisation}

Ground points were classified using the Cloth Simulation Filtering (CSF) algorithm, as described in \cite{zhang2016}. Points were already classified in the data provided, however, as the classification technique was unknown, reclassification was considered necessary. The general implementation simulates the movements of a piece of cloth lying over the inverse of a point cloud, as the point cloud is flipped, the cloth settles beneath ground points, while covering points that lie separate to the ground, essentially forming a digital terrain model (DTM), cloth simulations are described in more detail in \cite{bridson2005}. The CSF algorithm is given;

$$
X(t+\Delta t)=2 X(t)-X(t-\Delta t)+\frac{G}{m} \Delta t^{2},
$$
\begin{flushright}
    \footnotesize{\ref{code:lidr_clean}}
\end{flushright}

where $m$  is the mass of a single LiDAR point (set to 1), $\Delta t$ is the time step between points and $G$ represents the gravity constant.

With the classification of ground points, (given $Classification = 2$), a full DTM may be produced through spatial interpolation of the classified points. This process is called normalisation, and ensures that when extracting height information, any observed values are due to objects on the surface of the terrain, and not due to the lie of the terrain itself. Interpolation uses the inverse distance weighting and $k$ nearest neighbours algorithms to produce the DTM. Nearest neighbours were selected as $k = 10$, with $p = 2$ for the inverse weighting, and used to produce a DTM with a resolution of 1m. This particular technique was selected over more comprehensive methods such as kriging as the number of points is very high, and the small benefit was considered minimal compared with the increase in computational load. The $z$ values from the DTM were then subtracted from the LiDAR point cloud, leaving a normalised point cloud.

\subsection{Points Extent}

With the normalised last pulse point cloud, the point cloud was clipped to within a 30m extent of each known road location, using the OS road shapefiles. Selecting a 30m extent ensured that even with slight inaccuracy in road location, the road LiDAR points would likely not be excluded. A large number of unimportant points were therefore removed, saving on computational resources. Additionally this extent ensured that both road and non road points were included, but reduced the chance of false positives from occuring as fewer non road points were now included in the analysis.

\section{Road Analysis}
\label{sec:road-extraction}

This section combines data extracted through the OS road shapefiles preprocessed LiDAR data, and aerial imagery to obtain a set of creteria to assess the chosen road network. These criteria are;

\begin{itemize}
\itemsep0em
    \item Road Width
    \item Bend Sharpness
    \item Road Steepness
    \item Surface Quality
\end{itemize}


<<>>=
roads_info <- fread("/home/cjber/drive/uni/envs492/main/data/osroads/roads_info.csv") %>% 
select(-c(geometry, length_uom, name1_lang, name2_lang))

tmp <- do.call(
  data.frame,
  list(
    Max = apply(roads_info, 2, max)
  )
)
roads_info <- t(roads_info[1, ])
@

<<roadstab, results='asis'>>=
make_table(roads_info, cap = "OS Roads Data Summary",
           col.names = c("Variable", "Example")) %>%
  column_spec(2, width = "15em") %>%
  kable_styling(latex_options = "HOLD_position")
@

\subsection{Road Sampling}

The LiDAR point cloud data was sampled at regular 10 meter intervals for each road, perpendicular to the road direction, ensuring that when road directin changed, the sampling locations remained perpendicular. Each road was first split into nodes at which road direction changes, and from this, points at 10 meter intervals between two connected nodes were calculated;
$$
p_k = p_1 + |p_2 - p_1| \times I_k
$$

where $I_k$ is the interval value which increases by 10 meters until the length of the total distance of the node, given $I_{1}, I_{2}, \dots, I_{k}>0 and k \geq 2$.

\[
\begin{aligned}
    I_k &= (L_k + 20) - T_k \\
    L_1 &= 20 \\
    T_k &= T_{k-1}  + E
\end{aligned}
\]

where $L_k$ is the distance from the start of the node to the current sample line position, $T_k$ is total length of all nodes within a road from the start of the road and $E$ is the euclidean distance between two nodes.

To calculate the sample locations, perpendicular to the roads, first the $x$ and $y$ euclidean distances from a reference point $p_k$ and the end of the current node $n_2$ were calculated;

\[
\begin{aligned}
    len &= \sqrt{(n_{k+1} - p_k)} \\
    len_n &= (n / len) \times (n_{k+1} - p_k)
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:perp}}
\end{flushright}

from this, the lengths were inverted, giving two points of distance $n$ from $p_k$. Inversion was achieved by subtracting the given $len_n$ $x$ value from $p_k$, while adding the $len_n$ $y$ value, and vice versa. These perpendicular sample lines were buffered to a width of 1m, giving a total area of 20m$^2$ per sample and spatially joined to the existing LiDAR point cloud data, removing any points that fell outside the sample lines.

\begin{flushright}
    \footnotesize{\ref{code:compute_samples}}
\end{flushright}

\section{Aerial Imagery}

With the perpendicular sample lines extracted for the length of every road, to assist with the prediction of correct road locations, true colour aerial imagery was included. This imagery was first converted from three band RGB raster images, to a single-band grey-scale raster brick with values ranging 0 to 255, considered to be luminescence information.

$$
    luminescence = (Band_1 + Band_2 + Band_3) \div 3
$$
\begin{flushright}
    \footnotesize{\ref{code:lm_compute}}
\end{flushright}

To exclude as much noise as possible from both the LiDAR point cloud intensity information, and aerial luminescence, only points with the ground classification ($Classification = 2$), and points with only a single laser return pulse were included in the models ($NumberOfReturns = 1$).

\subsection{Linear Probability Model}

Individual linear probability models were constructed for each sample location for each road, giving results for a cluster of points across a perpendicular segment of each road every 20m. Road centrelines were selected as the outcome variable, provided by the OS road network line shapefiles, buffered to 1m to ensure a three dimensional cross section of points. Additionally, global linear probability models were constructed, and compared against the individual linear models.

Models were constructed using a maximal approach, selecting all appropriate predictor variables, iterating through models by removing variables that did not significantly influence the model outcome, or created noise.

An additional variable $Dist$ was included, representing the shortest distance from a point to the centreline of the road it is associated with, considering that road points should be weighted more towards points that are closer to the centre-point of the road.

Predictions for each point were then ran using each model, which gave the likelihood a point was either road or non road. As the likelihood values from the predictions gave a range of numerical values, points that fell below a certain threshold were removed, leaving only points that were most likely correctly identified as road points. This threshold was selected as the $95\%$ quantile for each sample point;

\[
\begin{aligned}
\mathbf{S} &= \Big(\mathbf{p}_i \in \Big[\frac{95}{100} \times \mathbf{p}_{i_{lpi}}\Big]\Big) \\
\end{aligned}
\]

Where $\mathbf{S}$ is the total point cloud.

Some points considered to be noise were still present, but often isolated. To ensure no isolated points were present, the time interval between two neighbouring points was checked, if a large time interval was found, the isolated point was removed.

$\mathbf{S}_i$ now consisted of a collection of predicted road points for each sample line along a road segment, excluding sample lines partially or fully obscured by tree canopy. To obtain road widths from these points, the maximum distance between two points in a particular sample was determined, these points were kept and all others removed. A linear section of road with two samples resembles \ref{fig:sample_points};

\begin{center}
\begin{tikzpicture}[scale=2]

\draw[-, very thick, color = gray] (0,0.5) -- (0,4);

\filldraw (-1,1) circle[radius=1pt] node [below left] {$A_1$};
\draw[-] (-1,1) -- (1,2);
\draw[dashed] (-1,1) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$O_A$} (1,1) ;
\draw[dotted] (1,1) -- (1,2);
\filldraw (1,2) circle[radius=1pt] node [above right] {$A_2$};

\draw(.85,1)--(.85,1.15)--(1,1.15); % right angle

\draw[<-] (-.6,1) arc (0:55:.2);
\node[] at (-.5,1.12)  {$\theta_A$};

\filldraw (-1,4) circle[radius=1pt] node [below left] {$B_1$};
\draw[-] (-1,4) -- (1,3);
\draw[dashed] (-1,3) -- node[fill=white,inner ysep=3pt, inner xsep = 3pt]{$O_B$} (1,3);
\draw[dotted] (-1,3) -- (-1,4);
\filldraw (1,3) circle[radius=1pt] node [above right] {$B_2$};

\draw(-.85,3)--(-.85,3.15)--(-1,3.15); % right angle

\draw[->] (.6,3.2) arc (135:200:.2);
\node[] at (.4,3.12)  {$\theta_B$};

\end{tikzpicture}
\end{center}
\captionof{figure}{Road LiDAR points at maximum distance apart for each sample location. Showing two example sample locations ($A$ and $B$), road centreline represented by the thick grey line. True road width is indicated by the dashed lines $O_A$ and $O_B$.}



To determine the width of the road section, according to the final two selected points at every node, pythagoras could be used to find the opposite line length, perpendicular to the road segement, considering the distance between the two points to be the hypoenuse of a right angled triangle (\ref{fig:sample_points}). The averge width for each road identifier was then found.

$|K_1K_2| = Hypotenuse$, $O_K = Opposite$, therefore $O_K = |K_1K_2| \times cos(\theta_K)$

\section{Road Angles}

The angle of bends in roads were identified through the nodes produced in the creation of the road shapefiles. First the road linestrings were split into points, with coordinates representing each node within a road, a point at which the orientation of the linestring is altered.

The bend in a road was considered to be the \textit{bearing angle} $\theta$, from a point $A\left(a_{x}, a_{y}\right)$ to a point $B\left(b_{x}, b_{y}\right)$, with the angle measured in a clockwise direction from north. This can be represented as a figure;

\begin{center}
\begin{tikzpicture}[scale=7]

\filldraw (.2,.4) circle[radius=.3pt] node [below left] {$A$};

\draw[-] (0.1,.4) -- (0.3, .4);
\draw[-] (0.2,.3) -- (0.2, .5) node[above] {$N$};

\filldraw (.8,.7) circle[radius=.3pt] node [above right] {$B$};

\draw[-] (0.2,.4) -- (0.8, .7);

    \draw[<-] (0.28,.44) arc (45:90:.111);
    \node[] at (.25,.5)  {$\theta$};
\end{tikzpicture}
\end{center}
\captionof{figure}{Bearing Angle Between Road Segments}

To find the angle $\theta$, the point $B$ can be represented into relation to point $A$ as;

$$
\left(b_{1}, b_{2}\right)=\left(a_{1}+r \sin \theta, a_{2}+r \cos \theta\right)
$$

Where $r$ is the length of the line segment $AB$. Rearranging the equation for $\theta$ gives;

$$
    \tan \theta=\frac{b_{1}-a_{1}}{b_{2}-a_{2}}
$$

This equation can be rewritten to calculate the value of $\theta$ using the $\mathit{atan2}$ function;

$$
\hat{\theta}=\mathrm{atan} 2\left(b_{1}-a_{1}, b_{2}-a_{2}\right) \in(-\pi, \pi]
$$


Finally the bearing angle $\theta \in[0,2 \pi)$ is given as;

\[
    \begin{aligned}
\theta=\left\{\begin{array}{ll}{\hat{\theta},} & {\hat{\theta} \geq 0} \\
{2 \pi+\hat{\theta},} & {\hat{\theta}<0}\end{array}\right.
\end{aligned}
\]
\begin{flushright}
    \footnotesize{\ref{code:angles}}
\end{flushright}

With the bearing angle of the first line segment $AB$ for a particular road, the change in orientation of the second road segment $BC$ may be given as $\theta_{2} = \theta_{BC} - \theta_{AB}$, with additional nodes following the pattern $\theta_{k} = \theta_{N(N+1)} - \theta_{(N-1)N}$.

\begin{center}
\begin{tikzpicture}[scale=7]

\filldraw (.2,.4) circle[radius=.3pt] node [below left] {$A$};

\draw[-] (0.1,.4) -- (0.3, .4);
\draw[-] (0.2,.3) -- (0.2, .5) node[above] {$N$};

\filldraw (.8,.7) circle[radius=.3pt] node [above left] {$B$};
\filldraw (.9,1) circle[radius=.3pt] node [above right] {$C$};

\draw[-] (0.2,.4) -- (0.8, .7);
\draw[-] (0.8,.7) -- (.9, 1);

    \draw[<-] (0.28,.44) arc (45:90:.111);
    \node[] at (.25,.5)  {$\theta_{1}$};

\draw[dotted] (0.8,.7) -- (1, .8);

    \draw[<-] (0.9,.75) arc (45:90:.1);
    \node[] at (.88,.8)  {$\theta_{2}$};
\end{tikzpicture}
\end{center}
\captionof{figure}{Bearing Angle Between two Road Segments}

For each road the maximum bearing angle between two nodes was selected, as well as the average bearing angle for a certain road.

\subsection{Road Node Elevation Change}

The elevation change between two road node points was calculated by first selecting non-normalised LiDAR points at a node points within a 1m$^2$ area. Points were then filtered by those only classified as ground, and with only a single return, to reduce the chance of inaccurate $z$ values. With several filtered points for each node, the mean $z$ value was found for each node, and elevation change between each node was calculated. For each road the maximum elevation change was calculated, alongside the mean elevation change.

\subsection{Surface Quality}

Surface quality was assessed roughly through the range in intensity values found in each known road point, and the average number of returns for a road.

\end{multicols}
